{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š KSA Transition Risk Analysis â€” Cement Sector Model\n",
    "\n",
    "**Author:** Saveeza Aziz  \n",
    "**Version:** 2.0 (Production-Ready)  \n",
    "**Objective:** This notebook performs a quantitative stress test on the Saudi Arabian cement sector to model the financial impact of carbon pricing. It serves as the analytical engine for the findings presented in the main article.\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Setup & Configuration:** Loads all dependencies, establishes a robust project path, and loads the master `config.yaml` file.\n",
    "2. **Data Loading & Validation:** Ingests and validates all source data, ensuring integrity before processing.\n",
    "3. **Analysis Functions:** Defines the core vectorized functions for the stress test.\n",
    "4. **Main Execution:** Orchestrates the full pipeline from loading to saving results.\n",
    "5. **Self-Testing:** Includes a simple smoke test to ensure the pipeline runs without error.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Preliminaries\n",
    "\n",
    "This cell handles all initial setup, including importing libraries, configuring the professional logging framework, and robustly locating the project's root directory and configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dependency Guard ---\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    import os\n",
    "    import logging\n",
    "    from datetime import datetime, timezone\n",
    "    import yaml\n",
    "    from typing import Dict, Optional, Tuple\n",
    "except ImportError as e:\n",
    "    print(f\"FATAL ERROR: A critical library is missing: {e}. Please install requirements.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Conditional Import for Display ---\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    IS_JUPYTER = True\n",
    "except ImportError:\n",
    "    IS_JUPYTER = False\n",
    "    def display(df: pd.DataFrame):\n",
    "        \"\"\"Fallback display function for non-Jupyter environments.\"\"\"\n",
    "        print(df.head(20).to_string())\n",
    "\n",
    "# --- Centralized Constants for Magic Strings ---\n",
    "class ConfigKeys:\n",
    "    PATHS = 'paths'\n",
    "    DATA_DIR = 'data_dir'\n",
    "    RESULTS_DIR = 'results_dir'\n",
    "    FILES = 'files'\n",
    "    ARCHETYPES_FILE = 'archetypes_file'\n",
    "    ASSUMPTIONS_FILE = 'assumptions_file'\n",
    "    CONSTANTS = 'constants'\n",
    "    SECTORS = 'sectors'\n",
    "    CEMENT_SECTOR = 'cement'\n",
    "    SHEET_NAME = 'sheet_name'\n",
    "    REQUIRED_COLS = 'required_columns'\n",
    "    METADATA = 'metadata'\n",
    "    VERSION = 'version'\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "# --- Robust Path & Configuration Management ---\n",
    "def find_project_root(anchor: str = '.git') -> Path:\n",
    "    \"\"\"Finds project root by searching upwards for an anchor, with fallbacks.\"\"\"\n",
    "    if 'PROJECT_ROOT' in os.environ:\n",
    "        return Path(os.environ['PROJECT_ROOT'])\n",
    "    path = Path.cwd()\n",
    "    while path.parent != path:\n",
    "        if (path / anchor).exists(): return path\n",
    "        path = path.parent\n",
    "    if (Path.cwd() / 'config.yaml').exists():\n",
    "        return Path.cwd()\n",
    "    raise FileNotFoundError(\"Project root not found. Set PROJECT_ROOT env var or run from repo root.\")\n",
    "\n",
    "try:\n",
    "    BASE_DIR = find_project_root()\n",
    "    with open(BASE_DIR / 'config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        if not isinstance(config, dict):\n",
    "            raise ValueError(\"config.yaml is empty or malformed.\")\n",
    "\n",
    "    RESULTS_DIR = BASE_DIR / config[ConfigKeys.PATHS][ConfigKeys.RESULTS_DIR]\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    LOG_FILE_PATH = RESULTS_DIR / f\"analysis_log_{datetime.now(timezone.utc).strftime('%Y%m%d')}.log\"\n",
    "    \n",
    "    file_handler = logging.FileHandler(LOG_FILE_PATH, encoding='utf-8')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    CONSTANTS = {k: int(v) for k, v in config.get(ConfigKeys.CONSTANTS, {}).items()}\n",
    "    REQUIRED_CONSTANTS = ['tons_per_megaton', 'sar_per_billion']\n",
    "    missing_consts = [c for c in REQUIRED_CONSTANTS if c not in CONSTANTS]\n",
    "    if missing_consts:\n",
    "        raise ValueError(f\"Missing required constants in config.yaml: {missing_consts}\")\n",
    "    \n",
    "    logger.info(f\"Project root found and configured: {BASE_DIR}\")\n",
    "\n",
    "except (FileNotFoundError, yaml.YAMLError, KeyError, ValueError) as e:\n",
    "    logging.critical(f\"FATAL ERROR during initial setup: {e}\")\n",
    "    raise SystemExit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Validation Functions\n",
    "\n",
    "These functions handle the ingestion of source files (`.xlsx`, `.csv`) and perform rigorous validation to ensure data integrity before any analysis is run. This includes checking for missing files, columns, and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Loads and validates all data sources based on the global config.\"\"\"\n",
    "    logger.info(\"Starting data loading and validation...\")\n",
    "    \n",
    "    data_dir = BASE_DIR / config[ConfigKeys.PATHS][ConfigKeys.DATA_DIR]\n",
    "    excel_path = data_dir / config[ConfigKeys.FILES][ConfigKeys.ARCHETYPES_FILE]\n",
    "    csv_path = data_dir / config[ConfigKeys.FILES][ConfigKeys.ASSUMPTIONS_FILE]['path']\n",
    "    \n",
    "    if not excel_path.exists() or not csv_path.exists():\n",
    "        raise FileNotFoundError(\"Input file(s) not found.\")\n",
    "\n",
    "    assumptions_df = pd.read_csv(csv_path, index_col='Parameter', encoding='utf-8-sig')\n",
    "    if 'Value' not in assumptions_df.columns:\n",
    "        raise KeyError(\"Column 'Value' missing in assumptions file.\")\n",
    "    if assumptions_df.index.duplicated().any():\n",
    "        raise ValueError(f\"Duplicate parameters found: {assumptions_df.index[assumptions_df.index.duplicated()].unique().tolist()}\")\n",
    "\n",
    "    xls = pd.ExcelFile(excel_path, engine='openpyxl')\n",
    "    sheet_name = config[ConfigKeys.SECTORS][ConfigKeys.CEMENT_SECTOR][ConfigKeys.SHEET_NAME]\n",
    "    if sheet_name not in xls.sheet_names:\n",
    "        raise ValueError(f\"Sheet '{sheet_name}' not found in Excel file.\")\n",
    "    cement_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "    \n",
    "    required_cols = config[ConfigKeys.SECTORS][ConfigKeys.CEMENT_SECTOR][ConfigKeys.REQUIRED_COLS]\n",
    "    if not all(col in cement_df.columns for col in required_cols):\n",
    "        raise ValueError(f\"Required columns missing: {set(required_cols) - set(cement_df.columns)}\")\n",
    "        \n",
    "    logger.info(\"âœ… Data files loaded successfully.\")\n",
    "    return cement_df, assumptions_df\n",
    "\n",
    "def process_and_validate_inputs(cement_df: pd.DataFrame, assumptions_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Cleans and validates dataframes before analysis.\"\"\"\n",
    "    logger.info(\"Processing and validating inputs...\")\n",
    "    \n",
    "    assumptions_df['Value'] = pd.to_numeric(assumptions_df['Value'], errors='coerce')\n",
    "    \n",
    "    required_params = ['FX_Rate_USD_SAR', 'Carbon_Cost_Pass_Through_Rate', 'CCUS_CAPEX_Cost']\n",
    "    missing_params = [p for p in required_params if p not in assumptions_df.index or pd.isna(assumptions_df.at[p, 'Value'])]\n",
    "    if missing_params:\n",
    "        raise ValueError(f\"Missing required assumptions: {missing_params}\")\n",
    "    \n",
    "    cement_df['Debt_to_EBITDA_Ratio'] = pd.to_numeric(cement_df['Debt_to_EBITDA_Ratio'].astype(str).replace('x', '', regex=False), errors='coerce')\n",
    "    n_debt_nan = cement_df['Debt_to_EBITDA_Ratio'].isna().sum()\n",
    "    if n_debt_nan > 0:\n",
    "        logger.warning(f\"{n_debt_nan} 'Debt_to_EBITDA_Ratio' values could not be coerced to numeric and were set to NaN.\")\n",
    "        if n_debt_nan / len(cement_df) > 0.3:\n",
    "            raise ValueError(\"Over 30% of Debt_to_EBITDA_Ratio values are invalid, halting analysis.\")\n",
    "\n",
    "    cement_df['Total_Debt_SAR_bn'] = cement_df['Reported_EBITDA_SAR_bn'] * cement_df['Debt_to_EBITDA_Ratio']\n",
    "    \n",
    "    logger.info(\"âœ… Inputs processed and validated successfully.\")\n",
    "    return cement_df, assumptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 3. Analysis & Export Functions\n",
    "\n",
    "This section contains the core analytical engine. The `run_analysis` function performs the vectorized stress test based on the validated inputs and scenarios. The `save_results` function handles the clean export of the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(cement_df: pd.DataFrame, assumptions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Runs the vectorized stress test and CAPEX analysis.\"\"\"\n",
    "    logger.info(\"Running core analysis...\")\n",
    "    results_df = cement_df.copy()\n",
    "    \n",
    "    def get_param(param_name: str, required: bool = True):\n",
    "        if param_name not in assumptions_df.index or pd.isna(assumptions_df.at[param_name, 'Value']):\n",
    "            if required: raise ValueError(f\"Required assumption '{param_name}' is missing or invalid.\")\n",
    "            return None\n",
    "        return assumptions_df.at[param_name, 'Value']\n",
    "\n",
    "    fx_rate = get_param('FX_Rate_USD_SAR')\n",
    "    pass_through = get_param('Carbon_Cost_Pass_Through_Rate')\n",
    "    ccus_cost = get_param('CCUS_CAPEX_Cost')\n",
    "    \n",
    "    carbon_df = assumptions_df.filter(like='Carbon_Price_')\n",
    "    if carbon_df.empty:\n",
    "        raise ValueError(\"No 'Carbon_Price_' scenarios found in assumptions file.\")\n",
    "    scenarios = carbon_df['Value'].to_dict()\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        for param_name, carbon_price in scenarios.items():\n",
    "            scenario_name = param_name.replace('Carbon_Price_', '').replace('_Scenario', '')\n",
    "            total_cost = (results_df['Scope_1_2_Emissions_MtCO2'] * CONSTANTS['tons_per_megaton'] * carbon_price * fx_rate) / CONSTANTS['sar_per_billion']\n",
    "            net_cost = total_cost * (1 - pass_through)\n",
    "            \n",
    "            stressed_col = f'Stressed_EBITDA_{scenario_name}'\n",
    "            results_df[stressed_col] = results_df['Reported_EBITDA_SAR_bn'] - net_cost\n",
    "            \n",
    "            leverage = (results_df['Total_Debt_SAR_bn'] / results_df[stressed_col]).replace([np.inf, -np.inf], np.nan)\n",
    "            results_df.loc[:, f'Stressed_Leverage_{scenario_name}'] = leverage\n",
    "\n",
    "    logger.info(\"âœ… Analysis complete.\")\n",
    "    return results_df\n",
    "\n",
    "def save_results(df: pd.DataFrame, config: Dict):\n",
    "    \"\"\"Saves the results dataframe to a versioned and timestamped CSV.\"\"\"\n",
    "    version = config.get(ConfigKeys.METADATA, {}).get(ConfigKeys.VERSION, '0.0.0')\n",
    "    timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'cement_stress_test_results_v{version}_{timestamp}.csv'\n",
    "    output_path = RESULTS_DIR / filename\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    logger.info(f\"Results successfully exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-header",
   "metadata": {},
   "source": [
    "## 4. Main Execution Script\n",
    "\n",
    "This section contains the `main()` function which orchestrates the entire pipeline, calling the validation, processing, and analysis functions in the correct order. It also handles top-level error catching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Orchestrates the entire analysis pipeline from data loading to saving results.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: Final results dataframe if successful, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_raw, assumptions_raw = load_and_validate_data()\n",
    "        df_clean, assumptions_clean = process_and_validate_inputs(df_raw, assumptions_raw)\n",
    "        final_results = run_analysis(df_clean, assumptions_clean)\n",
    "        save_results(final_results, config)\n",
    "        \n",
    "        if IS_JUPYTER:\n",
    "            logger.info(\"Displaying first 20 rows of results in Jupyter.\")\n",
    "            display(final_results.head(20).fillna('Insolvent'))\n",
    "        \n",
    "        logger.info(\"--- Pipeline finished successfully. ---\")\n",
    "        return final_results\n",
    "        \n",
    "    except Exception:\n",
    "        logger.exception(\"FATAL ERROR: Pipeline failed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution-header",
   "metadata": {},
   "source": [
    "## 5. Script Execution & Self-Testing\n",
    "\n",
    "The final block executes the `main()` function. It also contains a simple 'smoke test' function that can be uncommented to verify the pipeline's integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execution-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline_execution():\n",
    "    \"\"\"Simple 'smoke test' to ensure the main pipeline runs without crashing.\"\"\"\n",
    "    logger.info(\"\\n--- RUNNING SMOKE TEST ---\")\n",
    "    result = main()\n",
    "    if result is None or not isinstance(result, pd.DataFrame) or result.empty:\n",
    "        raise AssertionError(\"Smoke Test FAILED: Pipeline did not return a valid DataFrame.\")\n",
    "    logger.info(\"âœ… SMOKE TEST PASSED: Pipeline executed and returned a valid DataFrame.\")\n",
    "\n",
    "# This standard Python construct ensures main() is called only when the script is executed directly.\n",
    "if __name__ == \"__main__\":\n",
    "    # To run the main analysis:\n",
    "    main_results = main()\n",
    "    \n",
    "    # To run the self-test (uncomment the line below and comment out the line above):\n",
    "    # try:\n",
    "    #     test_pipeline_execution()\n",
    "    # except AssertionError as e:\n",
    "    #     logger.error(e)\n",
    "    #     sys.exit(1)\n",
    "        \n",
    "    if main_results is None:\n",
    "        # Exit with a non-zero status code to indicate failure in automated environments.\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}